{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82500b8",
   "metadata": {},
   "source": [
    "* we will replicate the code in the section 6 `PyTorch Custom Dataset` and transfrom it into modulers `.py` files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ee6eb",
   "metadata": {},
   "source": [
    "# 1. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc0e123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\pizza_steak_sushi directory already exists, skipping download...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "TARGET_URL = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\"\n",
    "\n",
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "file_path = data_path / TARGET_URL.split(\"/\")[-1]\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory already exists, skipping download...\")\n",
    "    \n",
    "else:\n",
    "    print(f\"{image_path} does not exist, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(file_path, \"wb\") as f:\n",
    "        response = requests.get(TARGET_URL)\n",
    "        print(\"Downloading pizza steak sushi data...\")\n",
    "\n",
    "        f.write(response.content)\n",
    "\n",
    "    with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "        print(\"Unzipping pizza steak sushi data...\")\n",
    "        zip_ref.extractall(image_path)\n",
    "        \n",
    "    os.remove(file_path) # Remove the folder after unzipping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3801f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi/train'),\n",
       " WindowsPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup train & testing paths\n",
    "train_dir =  image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d1a4d2",
   "metadata": {},
   "source": [
    "# 2. Create Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f7cb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      " Dataset ImageFolder\n",
      "    Number of datapoints: 225\n",
      "    Root location: data\\pizza_steak_sushi\\train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n",
      "Testing Data:\n",
      " Dataset ImageFolder\n",
      "    Number of datapoints: 75\n",
      "    Root location: data\\pizza_steak_sushi\\test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=data_transform,\n",
    "    target_transform=None,\n",
    ")\n",
    "\n",
    "test_data = datasets.ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=data_transform,\n",
    "    target_transform=None,\n",
    ")\n",
    "\n",
    "print(f\"Train Data:\\n {train_data}\\nTesting Data:\\n {test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81857278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1590b03de90>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1590b03e450>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import multiprocessing\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87555ee7",
   "metadata": {},
   "source": [
    "# 2.1 Create Datasets & DataLoaders (Script Mode)\n",
    "\n",
    "* We  will use the Jupyter magic functions to create a `.py` file for creating dataloaders.\n",
    "* We can save a code cell's contents to a file using the Jupyter magic `%%writefile filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec46a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatr a folder which will contain our moduler\n",
    "import os\n",
    "os.makedirs(\"modulers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ba8d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modulers\\data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"modulers\\data_setup.py\"\n",
    "import multiprocessing\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# NUM_WORKERS = multiprocessing.cpu_count()\n",
    "\n",
    "def create_dataloader(\n",
    "    train_dir: str,\n",
    "    test_dir: str,\n",
    "    transform: transforms.Compose,\n",
    "    batch_size: int,\n",
    "#     NUM_WORKERS: int = NUM_WORKERS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates DataLoader objects for training and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        train_dir (str): Path to the directory containing training images.\n",
    "        test_dir (str): Path to the directory containing testing images.\n",
    "        transform (transforms.Compose): A torchvision transforms.Compose object containing the transformations to apply to the images.\n",
    "        NUM_WORKERS (int, optional): Number of worker processes to use for data loading. Defaults to the number of available CPU cores.\n",
    "        batch_size (int): Number of images to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - train_loader (DataLoader): DataLoader for the training dataset.\n",
    "            - test_loader (DataLoader): DataLoader for the testing dataset.\n",
    "            - class_names (list): List of class names found in the training directory.\n",
    "\n",
    "    Example usage:\n",
    "        train_dir = \"/path/to/train\"\n",
    "        test_dir = \"/path/to/test\"\n",
    "        transform = transforms.Compose([...])\n",
    "        batch_size = 32\n",
    "        \n",
    "        train_loader, test_loader, class_names = create_dataloader(train_dir, test_dir, transform, batch_size=batch_size)\n",
    "    \"\"\"\n",
    "    # Read the image data from the Folders\n",
    "    train_data = datasets.ImageFolder(root=train_dir, transform=transform, )\n",
    "    test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "    \n",
    "    class_names = train_data.classes\n",
    "    \n",
    "    # Put the data into loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_data, \n",
    "        batch_size=batch_size, \n",
    "#         num_workers=NUM_WORKERS,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_data, \n",
    "        batch_size=batch_size, \n",
    "#         num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fe6e83f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import modulers.data_setup\n",
    "\n",
    "importlib.reload(modulers.data_setup)\n",
    "\n",
    "from modulers.data_setup import create_dataloader\n",
    "\n",
    "train_loader, test_loader, class_names = create_dataloader(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c8e4d",
   "metadata": {},
   "source": [
    "# 3. Making a model (TinyVGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f8113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TinyVGG(torch.nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block_1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv_block_2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(in_features=13*13*hidden_units, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.conv_block_2(self.conv_block_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f617b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_0 = TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(train_data.classes),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d2ad7",
   "metadata": {},
   "source": [
    "# 3.1. Making a model `TinyVGG` with a script `model_builder.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf0a759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modulers\\model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"modulers\\model_builder.py\"\n",
    "\n",
    "import torch\n",
    "    \n",
    "class TinyVGG(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a simple feedforward neural network model.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): The number of input features for the model.\n",
    "        hidden_units (int): The number of units in the hidden layer.\n",
    "        output_shape (int): The number of output units, typically corresponding to the number of classes for classification.\n",
    "\n",
    "    Example usage:\n",
    "        input_shape = 784\n",
    "        hidden_units = 128\n",
    "        output_shape = 10\n",
    "\n",
    "        model = TinyVGG(input_shape, hidden_units, output_shape)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block_1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block_2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(in_features=13*13*hidden_units, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.conv_block_2(self.conv_block_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96f282a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyVGG(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import modulers.model_builder\n",
    "\n",
    "importlib.reload(modulers.model_builder)\n",
    "\n",
    "from modulers.model_builder import TinyVGG\n",
    "\n",
    "model_1 = TinyVGG(\n",
    "    input_shape=3, # in my case it will be three, cause the color channel of my data is 3\n",
    "    hidden_units=10,\n",
    "    output_shape=len(train_data.classes),\n",
    ").to(device)\n",
    "\n",
    "model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def132c",
   "metadata": {},
   "source": [
    "# 4. Creating `train_step()`,  `test_step()` functions and `train()` to combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d01d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def train_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device=device\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # 1. Forward Pass\n",
    "        output = model(X) # outputs will be a logits\n",
    "        \n",
    "        # 2. Calculate the loss\n",
    "        loss = loss_fn(output, y)\n",
    "        train_loss += loss.item() # loss per batch\n",
    "        \n",
    "        #3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate the acc\n",
    "        y_pred_classes = torch.argmax(torch.softmax(output, dim=-1), dim=-1)\n",
    "        train_acc += ((y_pred_classes == y).sum() / len(y)).item() # acc per batch\n",
    "        \n",
    "#     adjust metrics to get all loss/accuracy values per all of the data\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    \n",
    "    return train_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f95bd885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device=device\n",
    ") -> Tuple[float, float]:\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            \n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "\n",
    "            loss = loss_fn(outputs, y)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            y_pred_classes = torch.argmax(torch.softmax(outputs, dim=-1), dim=-1)\n",
    "            test_acc += (y_pred_classes == y).sum().item() / len(y)\n",
    "            \n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= len(dataloader)\n",
    "    \n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3a2e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epochs: int = 5,\n",
    "    device: torch.device = device\n",
    ") -> Dict[str, List[float]]:\n",
    "#     Result dictionary\n",
    "    result = {\n",
    "        \"train_acc\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "    }\n",
    "#     loop throught train, and test step functions\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        train_acc, train_loss = train_step(model, train_dataloader, optimizer, loss_fn, device)\n",
    "        \n",
    "        test_acc, test_loss = test_step(model, test_dataloader, loss_fn, device)\n",
    "        \n",
    "#         print out the result\n",
    "        print(\n",
    "            f\"Epoch: {epoch} | Train acc: {train_acc:.4f}, Train Loss: {train_loss:.4f} | Test acc: {test_acc:.4f}, Test Loss: {test_loss:.4f}\"\n",
    "        )\n",
    "    \n",
    "        result[\"train_acc\"].append(train_acc)\n",
    "        result[\"train_loss\"].append(train_loss)\n",
    "        result[\"test_acc\"].append(test_acc)\n",
    "        result[\"test_loss\"].append(test_loss)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec106e8",
   "metadata": {},
   "source": [
    "# 4.1. Turn training functions into a script `engine.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bf27aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modulers\\engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"modulers\\engine.py\"\n",
    "\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "def train_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs a single training step for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to train.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing the training data in batches.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for updating the model's parameters.\n",
    "        loss_fn (torch.nn.Module): The loss function used to calculate the error between predictions and targets.\n",
    "        device (torch.device): The device (CPU or GPU) on which the computations are performed.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple containing:\n",
    "            - train_acc (float): The average training accuracy across all batches.\n",
    "            - train_loss (float): The average training loss across all batches.\n",
    "\n",
    "    Example usage:\n",
    "        train_acc, train_loss = train_step(model, train_dataloader, optimizer, loss_fn, device)\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # 1. Forward Pass\n",
    "        output = model(X) # outputs will be a logits\n",
    "        \n",
    "        # 2. Calculate the loss\n",
    "        loss = loss_fn(output, y)\n",
    "        train_loss += loss.item() # loss per batch\n",
    "        \n",
    "        #3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate the acc\n",
    "        y_pred_classes = torch.argmax(torch.softmax(output, dim=-1), dim=-1)\n",
    "        train_acc += ((y_pred_classes == y).sum() / len(y)).item() # acc per batch\n",
    "        \n",
    "#     adjust metrics to get all loss/accuracy values per all of the data\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    \n",
    "    return train_acc, train_loss\n",
    "\n",
    "\n",
    "def test_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the model on a validation or test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing the validation or test data in batches.\n",
    "        loss_fn (torch.nn.Module): The loss function used to calculate the error between predictions and targets.\n",
    "        device (torch.device): The device (CPU or GPU) on which the computations are performed.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple containing:\n",
    "            - test_acc (float): The average accuracy across all batches in the test dataset.\n",
    "            - test_loss (float): The average loss across all batches in the test dataset.\n",
    "\n",
    "    Example usage:\n",
    "        test_acc, test_loss = test_step(model, test_dataloader, loss_fn, device)\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            \n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "\n",
    "            loss = loss_fn(outputs, y)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            y_pred_classes = torch.argmax(torch.softmax(outputs, dim=-1), dim=-1)\n",
    "            test_acc += (y_pred_classes == y).sum().item() / len(y)\n",
    "            \n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= len(dataloader)\n",
    "    \n",
    "    return test_acc, test_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epochs: int,\n",
    "    device: torch.device\n",
    ") -> Dict[str, List[float]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains and evaluates the model over a specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to train and evaluate.\n",
    "        train_dataloader (torch.utils.data.DataLoader): DataLoader providing the training data in batches.\n",
    "        test_dataloader (torch.utils.data.DataLoader): DataLoader providing the test data in batches.\n",
    "        loss_fn (torch.nn.Module): The loss function used to calculate the error between predictions and targets.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for updating the model's parameters.\n",
    "        epochs (int, optional): The number of epochs to train the model. Defaults to 5.\n",
    "        device (torch.device, optional): The device (CPU or GPU) on which the computations are performed. Defaults to the global `device`.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[float]]: A dictionary containing lists of accuracy and loss values for both the training and test datasets across all epochs:\n",
    "            - \"train_acc\" (List[float]): Training accuracy for each epoch.\n",
    "            - \"train_loss\" (List[float]): Training loss for each epoch.\n",
    "            - \"test_acc\" (List[float]): Test accuracy for each epoch.\n",
    "            - \"test_loss\" (List[float]): Test loss for each epoch.\n",
    "\n",
    "    Example usage:\n",
    "        results = train(model, train_dataloader, test_dataloader, loss_fn, optimizer, epochs=10, device=device)\n",
    "    \"\"\"\n",
    "#     Result dictionary\n",
    "    result = {\n",
    "        \"train_acc\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "    }\n",
    "#     loop throught train, and test step functions\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        train_acc, train_loss = train_step(model, train_dataloader, optimizer, loss_fn, device)\n",
    "        \n",
    "        test_acc, test_loss = test_step(model, test_dataloader, loss_fn, device)\n",
    "        \n",
    "#         print out the result\n",
    "        print(\n",
    "            f\"Epoch: {epoch} | Train acc: {train_acc:.4f}, Train Loss: {train_loss:.4f} | Test acc: {test_acc:.4f}, Test Loss: {test_loss:.4f}\"\n",
    "        )\n",
    "    \n",
    "        result[\"train_acc\"].append(train_acc)\n",
    "        result[\"train_loss\"].append(train_loss)\n",
    "        result[\"test_acc\"].append(test_acc)\n",
    "        result[\"test_loss\"].append(test_loss)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0faf7cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_model(model: torch.nn.Module, target_dir: str, model_name:str):\n",
    "    \n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    assert model_name.endswith(\"pth\") or model_name.endswith(\"pt\"), \"model_name parameter should end with `pth` or `pt`.\"\n",
    "    model_path = target_dir_path / model_name\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    print(f\"The model was saved successfully in: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811b1ca3",
   "metadata": {},
   "source": [
    "# 5.1. Creat a file called `utils.py` with utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22faf541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modulers\\utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"modulers\\utils.py\"\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def save_model(\n",
    "    model: torch.nn.Module,\n",
    "    target_dir: str,\n",
    "    model_name:str\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves the model's state dictionary to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to save.\n",
    "        target_dir (str): The target directory where the model will be saved.\n",
    "        model_name (str): The name of the model file. Must end with '.pth' or '.pt'.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If `model_name` does not end with '.pth' or '.pt'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example usage:\n",
    "        save_model(model, target_dir=\"models/\", model_name=\"my_model.pth\")\n",
    "    \"\"\"\n",
    "    \n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    assert model_name.endswith(\"pth\") or model_name.endswith(\"pt\"), \"model_name parameter should end with `pth` or `pt`.\"\n",
    "    model_path = target_dir_path / model_name\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    print(f\"The model was saved successfully in: {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d175685",
   "metadata": {},
   "source": [
    "# 6. Train, Evaluate, and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08d3704e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e22a4e7bf0a402abcf85b7668f54077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train acc: 0.3047, Train Loss: 1.1063 | Test acc: 0.3116, Test Loss: 1.0983\n",
      "Epoch: 1 | Train acc: 0.3320, Train Loss: 1.0995 | Test acc: 0.5417, Test Loss: 1.0698\n",
      "Epoch: 2 | Train acc: 0.4922, Train Loss: 1.0862 | Test acc: 0.5227, Test Loss: 1.0799\n",
      "Epoch: 3 | Train acc: 0.4102, Train Loss: 1.0826 | Test acc: 0.5729, Test Loss: 1.0598\n",
      "Epoch: 4 | Train acc: 0.4141, Train Loss: 1.0631 | Test acc: 0.5540, Test Loss: 1.0612\n",
      "Epoch: 5 | Train acc: 0.4570, Train Loss: 1.0214 | Test acc: 0.4820, Test Loss: 1.0069\n",
      "Epoch: 6 | Train acc: 0.4258, Train Loss: 0.9661 | Test acc: 0.6042, Test Loss: 0.9268\n",
      "Epoch: 7 | Train acc: 0.5938, Train Loss: 0.9246 | Test acc: 0.4953, Test Loss: 1.0317\n",
      "Epoch: 8 | Train acc: 0.6172, Train Loss: 0.9082 | Test acc: 0.3532, Test Loss: 1.0718\n",
      "Epoch: 9 | Train acc: 0.4609, Train Loss: 0.9925 | Test acc: 0.4233, Test Loss: 1.0331\n",
      "Epoch: 10 | Train acc: 0.5312, Train Loss: 0.8468 | Test acc: 0.5540, Test Loss: 0.9897\n",
      "Epoch: 11 | Train acc: 0.4375, Train Loss: 0.9302 | Test acc: 0.5758, Test Loss: 0.9447\n",
      "Epoch: 12 | Train acc: 0.6523, Train Loss: 0.7830 | Test acc: 0.4129, Test Loss: 0.9990\n",
      "Epoch: 13 | Train acc: 0.5234, Train Loss: 0.9048 | Test acc: 0.4744, Test Loss: 1.0614\n",
      "Epoch: 14 | Train acc: 0.7109, Train Loss: 0.7725 | Test acc: 0.5142, Test Loss: 0.9987\n",
      "Epoch: 15 | Train acc: 0.6875, Train Loss: 0.7318 | Test acc: 0.5038, Test Loss: 1.0088\n",
      "Epoch: 16 | Train acc: 0.6992, Train Loss: 0.7491 | Test acc: 0.4337, Test Loss: 1.0337\n",
      "Epoch: 17 | Train acc: 0.6641, Train Loss: 0.7235 | Test acc: 0.4337, Test Loss: 1.0766\n",
      "Epoch: 18 | Train acc: 0.7188, Train Loss: 0.7452 | Test acc: 0.4848, Test Loss: 1.0634\n",
      "Epoch: 19 | Train acc: 0.6445, Train Loss: 0.7362 | Test acc: 0.4953, Test Loss: 1.0654\n",
      "Epoch: 20 | Train acc: 0.6055, Train Loss: 0.7826 | Test acc: 0.5256, Test Loss: 1.0748\n",
      "Epoch: 21 | Train acc: 0.6016, Train Loss: 0.7416 | Test acc: 0.5559, Test Loss: 1.0492\n",
      "Epoch: 22 | Train acc: 0.6992, Train Loss: 0.6845 | Test acc: 0.3627, Test Loss: 1.1524\n",
      "Epoch: 23 | Train acc: 0.5820, Train Loss: 0.8475 | Test acc: 0.4242, Test Loss: 1.1070\n",
      "Epoch: 24 | Train acc: 0.7422, Train Loss: 0.6291 | Test acc: 0.5350, Test Loss: 1.0301\n",
      "Epoch: 25 | Train acc: 0.7461, Train Loss: 0.6820 | Test acc: 0.5152, Test Loss: 1.0190\n",
      "Epoch: 26 | Train acc: 0.7383, Train Loss: 0.6061 | Test acc: 0.3939, Test Loss: 1.0957\n",
      "Epoch: 27 | Train acc: 0.7422, Train Loss: 0.6130 | Test acc: 0.4545, Test Loss: 1.1325\n",
      "Epoch: 28 | Train acc: 0.7656, Train Loss: 0.5715 | Test acc: 0.5161, Test Loss: 1.1662\n",
      "Epoch: 29 | Train acc: 0.6680, Train Loss: 0.7283 | Test acc: 0.4650, Test Loss: 1.1709\n",
      "Epoch: 30 | Train acc: 0.8125, Train Loss: 0.6256 | Test acc: 0.5265, Test Loss: 1.0815\n",
      "Epoch: 31 | Train acc: 0.8008, Train Loss: 0.5395 | Test acc: 0.4858, Test Loss: 1.2126\n",
      "Epoch: 32 | Train acc: 0.7930, Train Loss: 0.5450 | Test acc: 0.4044, Test Loss: 1.2716\n",
      "Epoch: 33 | Train acc: 0.7930, Train Loss: 0.5379 | Test acc: 0.4650, Test Loss: 1.2676\n",
      "Epoch: 34 | Train acc: 0.8047, Train Loss: 0.5128 | Test acc: 0.4242, Test Loss: 1.2520\n",
      "Epoch: 35 | Train acc: 0.7930, Train Loss: 0.5264 | Test acc: 0.5360, Test Loss: 1.3745\n",
      "Epoch: 36 | Train acc: 0.7852, Train Loss: 0.5418 | Test acc: 0.4034, Test Loss: 1.3450\n",
      "Epoch: 37 | Train acc: 0.8008, Train Loss: 0.5679 | Test acc: 0.4754, Test Loss: 1.4449\n",
      "Epoch: 38 | Train acc: 0.7695, Train Loss: 0.6437 | Test acc: 0.4347, Test Loss: 1.2876\n",
      "Epoch: 39 | Train acc: 0.7383, Train Loss: 0.6305 | Test acc: 0.4659, Test Loss: 1.2342\n",
      "Epoch: 40 | Train acc: 0.6562, Train Loss: 0.6814 | Test acc: 0.4451, Test Loss: 1.2398\n",
      "Epoch: 41 | Train acc: 0.7070, Train Loss: 0.6967 | Test acc: 0.5341, Test Loss: 1.1466\n",
      "Epoch: 42 | Train acc: 0.7852, Train Loss: 0.6235 | Test acc: 0.4953, Test Loss: 1.2137\n",
      "Epoch: 43 | Train acc: 0.6562, Train Loss: 0.6171 | Test acc: 0.3930, Test Loss: 1.3467\n",
      "Epoch: 44 | Train acc: 0.6602, Train Loss: 0.7079 | Test acc: 0.5559, Test Loss: 1.4866\n",
      "Epoch: 45 | Train acc: 0.6875, Train Loss: 0.6809 | Test acc: 0.5644, Test Loss: 1.2122\n",
      "Epoch: 46 | Train acc: 0.7930, Train Loss: 0.5758 | Test acc: 0.4953, Test Loss: 1.2260\n",
      "Epoch: 47 | Train acc: 0.6875, Train Loss: 0.7002 | Test acc: 0.5369, Test Loss: 1.3535\n",
      "Epoch: 48 | Train acc: 0.8359, Train Loss: 0.4630 | Test acc: 0.5256, Test Loss: 1.1755\n",
      "Epoch: 49 | Train acc: 0.8672, Train Loss: 0.4915 | Test acc: 0.4848, Test Loss: 1.2957\n",
      "Total training time: 91.6380 seconds\n",
      "The model was saved successfully in: modulers\\models\\going_moduler_cell_mode_tinyvgg_model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "model_0 = TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_0.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Start time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "# Train the model\n",
    "\n",
    "model_0_result = train(\n",
    "    model=model_0,\n",
    "    train_dataloader=train_loader,\n",
    "    test_dataloader=test_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# End the timer\n",
    "end = timer()\n",
    "\n",
    "print(f\"Total training time: {end - start:.4f} seconds\")\n",
    "\n",
    "# Save the model\n",
    "save_model(\n",
    "    model=model_0,\n",
    "    target_dir=\"modulers/models/\",\n",
    "    model_name=\"going_moduler_cell_mode_tinyvgg_model.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73bac6",
   "metadata": {},
   "source": [
    "# 6.1. Train, Evaluate, and save the model script mode `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00a92096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modulers\\train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"modulers\\train.py\"\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import data_setup, model_builder, engine, utils\n",
    "\n",
    "# Some Hyper-parameters\n",
    "NUM_EPOCHS = 50\n",
    "HIDDEN_UNITS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# SET THE DEVICE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Directories\n",
    "train_dir = 'data/pizza_steak_sushi/train'\n",
    "test_dir = 'data/pizza_steak_sushi/test'\n",
    "TARGET_MODEDL_PATH = \"modulers/model/\"\n",
    "\n",
    "# Create a tansform\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create our data loaders\n",
    "train_loader, test_loader, class_names = data_setup.create_dataloader(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Create a model\n",
    "model = model_builder.TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=HIDDEN_UNITS,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "# Setup loss and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Start time\n",
    "start = timer()\n",
    "\n",
    "# Start training with engine file\n",
    "result = engine.train(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    test_dataloader=test_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# End the timer\n",
    "end = timer()\n",
    "\n",
    "print(f\"Total training time: {end - start:.4f} seconds\")\n",
    "\n",
    "# Save the model\n",
    "utils.save_model(\n",
    "    model,\n",
    "    TARGET_MODEDL_PATH,\n",
    "    \"going_moduler_script_mode_tinyvgg_model.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "594043c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train acc: 0.2891, Train Loss: 1.1019 | Test acc: 0.2992, Test Loss: 1.0981\n",
      "Epoch: 1 | Train acc: 0.2969, Train Loss: 1.0996 | Test acc: 0.1979, Test Loss: 1.1110\n",
      "Epoch: 2 | Train acc: 0.2930, Train Loss: 1.0953 | Test acc: 0.1979, Test Loss: 1.1111\n",
      "Epoch: 3 | Train acc: 0.2930, Train Loss: 1.0910 | Test acc: 0.2396, Test Loss: 1.1229\n",
      "Epoch: 4 | Train acc: 0.4727, Train Loss: 1.1010 | Test acc: 0.2812, Test Loss: 1.1156\n",
      "Epoch: 5 | Train acc: 0.4531, Train Loss: 1.0736 | Test acc: 0.4631, Test Loss: 1.0699\n",
      "Epoch: 6 | Train acc: 0.4297, Train Loss: 1.0822 | Test acc: 0.6146, Test Loss: 1.0120\n",
      "Epoch: 7 | Train acc: 0.5625, Train Loss: 1.0164 | Test acc: 0.3106, Test Loss: 1.0926\n",
      "Epoch: 8 | Train acc: 0.5000, Train Loss: 1.0047 | Test acc: 0.3532, Test Loss: 1.0567\n",
      "Epoch: 9 | Train acc: 0.6602, Train Loss: 0.9735 | Test acc: 0.2812, Test Loss: 1.0367\n",
      "Epoch: 10 | Train acc: 0.6133, Train Loss: 0.9133 | Test acc: 0.3116, Test Loss: 1.0334\n",
      "Epoch: 11 | Train acc: 0.6211, Train Loss: 0.8392 | Test acc: 0.3011, Test Loss: 1.0687\n",
      "Epoch: 12 | Train acc: 0.4883, Train Loss: 0.9993 | Test acc: 0.4451, Test Loss: 1.0701\n",
      "Epoch: 13 | Train acc: 0.4883, Train Loss: 0.9785 | Test acc: 0.6061, Test Loss: 0.9530\n",
      "Epoch: 14 | Train acc: 0.6523, Train Loss: 0.8385 | Test acc: 0.5341, Test Loss: 0.9427\n",
      "Epoch: 15 | Train acc: 0.5391, Train Loss: 0.9225 | Test acc: 0.4735, Test Loss: 0.9587\n",
      "Epoch: 16 | Train acc: 0.5195, Train Loss: 0.9135 | Test acc: 0.4640, Test Loss: 1.0071\n",
      "Epoch: 17 | Train acc: 0.5195, Train Loss: 0.8523 | Test acc: 0.5246, Test Loss: 0.9350\n",
      "Epoch: 18 | Train acc: 0.7148, Train Loss: 0.7224 | Test acc: 0.4432, Test Loss: 0.9970\n",
      "Epoch: 19 | Train acc: 0.5742, Train Loss: 0.8375 | Test acc: 0.4432, Test Loss: 1.0034\n",
      "Epoch: 20 | Train acc: 0.6836, Train Loss: 0.7930 | Test acc: 0.4451, Test Loss: 1.0087\n",
      "Epoch: 21 | Train acc: 0.6914, Train Loss: 0.7155 | Test acc: 0.4848, Test Loss: 1.0036\n",
      "Epoch: 22 | Train acc: 0.7109, Train Loss: 0.7605 | Test acc: 0.4242, Test Loss: 0.9974\n",
      "Epoch: 23 | Train acc: 0.6523, Train Loss: 0.7313 | Test acc: 0.4953, Test Loss: 0.9891\n",
      "Epoch: 24 | Train acc: 0.7461, Train Loss: 0.7442 | Test acc: 0.5246, Test Loss: 0.9546\n",
      "Epoch: 25 | Train acc: 0.6953, Train Loss: 0.7611 | Test acc: 0.4848, Test Loss: 1.0270\n",
      "Epoch: 26 | Train acc: 0.7188, Train Loss: 0.6683 | Test acc: 0.4640, Test Loss: 0.9941\n",
      "Epoch: 27 | Train acc: 0.6367, Train Loss: 0.8778 | Test acc: 0.4432, Test Loss: 0.9823\n",
      "Epoch: 28 | Train acc: 0.5820, Train Loss: 0.8212 | Test acc: 0.4337, Test Loss: 1.0133\n",
      "Epoch: 29 | Train acc: 0.7930, Train Loss: 0.6044 | Test acc: 0.4943, Test Loss: 0.9418\n",
      "Epoch: 30 | Train acc: 0.7773, Train Loss: 0.5901 | Test acc: 0.4943, Test Loss: 0.9486\n",
      "Epoch: 31 | Train acc: 0.7812, Train Loss: 0.5531 | Test acc: 0.4754, Test Loss: 1.0883\n",
      "Epoch: 32 | Train acc: 0.7930, Train Loss: 0.6101 | Test acc: 0.4441, Test Loss: 1.0645\n",
      "Epoch: 33 | Train acc: 0.6562, Train Loss: 0.8636 | Test acc: 0.5057, Test Loss: 1.0322\n",
      "Epoch: 34 | Train acc: 0.7461, Train Loss: 0.6071 | Test acc: 0.4233, Test Loss: 1.0271\n",
      "Epoch: 35 | Train acc: 0.7773, Train Loss: 0.5835 | Test acc: 0.4441, Test Loss: 0.9646\n",
      "Epoch: 36 | Train acc: 0.8125, Train Loss: 0.5717 | Test acc: 0.4735, Test Loss: 0.9889\n",
      "Epoch: 37 | Train acc: 0.6914, Train Loss: 0.6200 | Test acc: 0.4441, Test Loss: 1.1043\n",
      "Epoch: 38 | Train acc: 0.6797, Train Loss: 0.6161 | Test acc: 0.4233, Test Loss: 1.1472\n",
      "Epoch: 39 | Train acc: 0.8320, Train Loss: 0.5030 | Test acc: 0.4536, Test Loss: 1.0288\n",
      "Epoch: 40 | Train acc: 0.8438, Train Loss: 0.4864 | Test acc: 0.4337, Test Loss: 1.1297\n",
      "Epoch: 41 | Train acc: 0.8398, Train Loss: 0.4277 | Test acc: 0.4545, Test Loss: 1.2268\n",
      "Epoch: 42 | Train acc: 0.8672, Train Loss: 0.4749 | Test acc: 0.4848, Test Loss: 1.1935\n",
      "Epoch: 43 | Train acc: 0.8125, Train Loss: 0.5295 | Test acc: 0.4034, Test Loss: 1.2866\n",
      "Epoch: 44 | Train acc: 0.8438, Train Loss: 0.4049 | Test acc: 0.3930, Test Loss: 1.2263\n",
      "Epoch: 45 | Train acc: 0.8750, Train Loss: 0.3898 | Test acc: 0.4640, Test Loss: 1.1827\n",
      "Epoch: 46 | Train acc: 0.9023, Train Loss: 0.3284 | Test acc: 0.4242, Test Loss: 1.3200\n",
      "Epoch: 47 | Train acc: 0.9023, Train Loss: 0.3169 | Test acc: 0.4138, Test Loss: 1.3800\n",
      "Epoch: 48 | Train acc: 0.9062, Train Loss: 0.2775 | Test acc: 0.4545, Test Loss: 1.3303\n",
      "Epoch: 49 | Train acc: 0.9102, Train Loss: 0.2653 | Test acc: 0.4555, Test Loss: 1.6264\n",
      "Total training time: 77.3260 seconds\n",
      "The model was saved successfully in: modulers\\model\\going_moduler_script_mode_tinyvgg_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 20:47:38.372740: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-30 20:47:39.397103: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-30 20:47:42.488248: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n",
      "  2%|2         | 1/50 [00:01<01:10,  1.43s/it]\n",
      "  4%|4         | 2/50 [00:02<01:10,  1.46s/it]\n",
      "  6%|6         | 3/50 [00:04<01:07,  1.43s/it]\n",
      "  8%|8         | 4/50 [00:05<01:06,  1.44s/it]\n",
      " 10%|#         | 5/50 [00:07<01:04,  1.44s/it]\n",
      " 12%|#2        | 6/50 [00:08<01:03,  1.43s/it]\n",
      " 14%|#4        | 7/50 [00:09<01:00,  1.41s/it]\n",
      " 16%|#6        | 8/50 [00:11<00:59,  1.41s/it]\n",
      " 18%|#8        | 9/50 [00:12<00:59,  1.44s/it]\n",
      " 20%|##        | 10/50 [00:14<00:58,  1.47s/it]\n",
      " 22%|##2       | 11/50 [00:15<00:56,  1.46s/it]\n",
      " 24%|##4       | 12/50 [00:17<00:55,  1.45s/it]\n",
      " 26%|##6       | 13/50 [00:18<00:53,  1.45s/it]\n",
      " 28%|##8       | 14/50 [00:20<00:52,  1.45s/it]\n",
      " 30%|###       | 15/50 [00:21<00:51,  1.47s/it]\n",
      " 32%|###2      | 16/50 [00:23<00:54,  1.59s/it]\n",
      " 34%|###4      | 17/50 [00:25<00:52,  1.58s/it]\n",
      " 36%|###6      | 18/50 [00:26<00:51,  1.61s/it]\n",
      " 38%|###8      | 19/50 [00:28<00:50,  1.63s/it]\n",
      " 40%|####      | 20/50 [00:30<00:49,  1.65s/it]\n",
      " 42%|####2     | 21/50 [00:31<00:47,  1.62s/it]\n",
      " 44%|####4     | 22/50 [00:33<00:45,  1.61s/it]\n",
      " 46%|####6     | 23/50 [00:34<00:42,  1.58s/it]\n",
      " 48%|####8     | 24/50 [00:36<00:40,  1.55s/it]\n",
      " 50%|#####     | 25/50 [00:37<00:38,  1.54s/it]\n",
      " 52%|#####2    | 26/50 [00:39<00:36,  1.54s/it]\n",
      " 54%|#####4    | 27/50 [00:41<00:37,  1.62s/it]\n",
      " 56%|#####6    | 28/50 [00:42<00:35,  1.62s/it]\n",
      " 58%|#####8    | 29/50 [00:44<00:33,  1.61s/it]\n",
      " 60%|######    | 30/50 [00:45<00:31,  1.60s/it]\n",
      " 62%|######2   | 31/50 [00:47<00:30,  1.61s/it]\n",
      " 64%|######4   | 32/50 [00:49<00:28,  1.61s/it]\n",
      " 66%|######6   | 33/50 [00:50<00:27,  1.64s/it]\n",
      " 68%|######8   | 34/50 [00:52<00:26,  1.65s/it]\n",
      " 70%|#######   | 35/50 [00:54<00:24,  1.64s/it]\n",
      " 72%|#######2  | 36/50 [00:55<00:22,  1.63s/it]\n",
      " 74%|#######4  | 37/50 [00:57<00:20,  1.60s/it]\n",
      " 76%|#######6  | 38/50 [00:58<00:19,  1.60s/it]\n",
      " 78%|#######8  | 39/50 [01:00<00:17,  1.60s/it]\n",
      " 80%|########  | 40/50 [01:02<00:15,  1.59s/it]\n",
      " 82%|########2 | 41/50 [01:03<00:14,  1.56s/it]\n",
      " 84%|########4 | 42/50 [01:05<00:12,  1.55s/it]\n",
      " 86%|########6 | 43/50 [01:06<00:10,  1.53s/it]\n",
      " 88%|########8 | 44/50 [01:08<00:09,  1.53s/it]\n",
      " 90%|######### | 45/50 [01:09<00:07,  1.54s/it]\n",
      " 92%|#########2| 46/50 [01:11<00:06,  1.55s/it]\n",
      " 94%|#########3| 47/50 [01:12<00:04,  1.53s/it]\n",
      " 96%|#########6| 48/50 [01:14<00:03,  1.54s/it]\n",
      " 98%|#########8| 49/50 [01:15<00:01,  1.52s/it]\n",
      "100%|##########| 50/50 [01:17<00:00,  1.52s/it]\n",
      "100%|##########| 50/50 [01:17<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "!python modulers/train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
